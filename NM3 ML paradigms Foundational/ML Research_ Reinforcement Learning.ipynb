{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEOQNgkArg59RIsmCnq6cm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This project is a hands-on project that implements a simple grid world environment and a Q-learning agent in Python using reinforcement learning. The agent learns to navigate the grid, avoiding pitfalls and reaching the goal."],"metadata":{"id":"NYUA9YL8kGfd"}},{"cell_type":"markdown","source":["# Step 1: Define the GridWorld class"],"metadata":{"id":"ynZxijzxkNga"}},{"cell_type":"markdown","source":["* The GridWorld class represents the environment the agent interacts with\n","* The environment has a grid structure with an agent, an exit, and a pitfall\n","* The agent can take actions ('up', 'down', 'left', 'right') to navigate the grid"],"metadata":{"id":"9ZDSBzw_kQoq"}},{"cell_type":"code","source":["import random\n","\n","class GridWorld:\n","    def __init__(self):\n","        # Define the grid world properties\n","        self.rows = 4\n","        self.cols = 4\n","        self.agent_position = (0, 0)\n","        self.exit_position = (self.rows - 1, self.cols - 1)\n","        self.pitfall_position = (1, 1)\n","\n","    def is_terminal(self):\n","        # Check if the agent is at the exit position\n","        return self.agent_position == self.exit_position\n","\n","    def is_pitfall(self):\n","        # Check if the agent is at a pitfall position\n","        return self.agent_position == self.pitfall_position\n","\n","    def take_action(self, action):\n","        # Update the agent's position based on the chosen action\n","        i, j = self.agent_position\n","        if action == 'up' and i > 0:\n","            self.agent_position = (i - 1, j)\n","        elif action == 'down' and i < self.rows - 1:\n","            self.agent_position = (i + 1, j)\n","        elif action == 'left' and j > 0:\n","            self.agent_position = (i, j - 1)\n","        elif action == 'right' and j < self.cols - 1:\n","            self.agent_position = (i, j + 1)\n","\n","    def get_reward(self):\n","        # Define the reward structure based on the agent's state\n","        if self.is_terminal():\n","            return 1\n","        elif self.is_pitfall():\n","            return -1\n","        else:\n","            return 0\n","\n","    def reset(self):\n","        # Reset the agent's position to the starting point\n","        self.agent_position = (0, 0)\n"],"metadata":{"id":"5P2Vh9Q_jdgP","executionInfo":{"status":"ok","timestamp":1701286673116,"user_tz":300,"elapsed":5,"user":{"displayName":"Eugene Tye","userId":"03176183298028220664"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Step 2: Define the QLearningAgent class"],"metadata":{"id":"E6QXTCIrkaQ1"}},{"cell_type":"markdown","source":["* The QLearningAgent class represents the Q-learning agent\n","* The agent has Q-values for state-action pairs, and it can choose actions based on an epsilon-greedy strategy\n","* The Q-values are updated using the Q-learning update rule"],"metadata":{"id":"85xBlkIJkd9S"}},{"cell_type":"code","source":["class QLearningAgent:\n","    def __init__(self, actions):\n","        # Initialize Q-learning agent properties\n","        self.actions = actions\n","        self.learning_rate = 0.1\n","        self.discount_factor = 0.9\n","        self.q_values = {}\n","\n","    def get_q_value(self, state, action):\n","        # Retrieve the Q-value for a state-action pair\n","        return self.q_values.get((state, action), 0.0)\n","\n","    def update_q_value(self, state, action, reward, next_state):\n","        # Update the Q-value based on the Q-learning update rule\n","        max_next_q = max([self.get_q_value(next_state, a) for a in self.actions])\n","        current_q = self.get_q_value(state, action)\n","        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * max_next_q)\n","        self.q_values[(state, action)] = new_q\n","\n","    def choose_action(self, state, epsilon):\n","        # Choose an action using epsilon-greedy exploration\n","        if random.uniform(0, 1) < epsilon:\n","            return random.choice(self.actions)\n","        else:\n","            q_values = [self.get_q_value(state, a) for a in self.actions]\n","            return self.actions[q_values.index(max(q_values))]\n"],"metadata":{"id":"CyUJzWGGi7Lw","executionInfo":{"status":"ok","timestamp":1701286673117,"user_tz":300,"elapsed":5,"user":{"displayName":"Eugene Tye","userId":"03176183298028220664"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Step 3: Train the Q-Learning Agent"],"metadata":{"id":"GjzI5lyIkzs7"}},{"cell_type":"markdown","source":["* Here, a function to train the Q-learning agent is defined\n","* It runs a specified number of episodes, where the agent interacts with the environment, chooses actions, and updates Q-values\n","* The total reward is printed every 100 episodes for monitoring"],"metadata":{"id":"HbUJ2eRck2pP"}},{"cell_type":"code","source":["def train_q_learning_agent(agent, env, epsilon, num_episodes):\n","    for episode in range(num_episodes):\n","        env.reset()\n","        total_reward = 0\n","\n","        while not env.is_terminal() and not env.is_pitfall():\n","            action = agent.choose_action(env.agent_position, epsilon)\n","            current_position = env.agent_position\n","            env.take_action(action)\n","            reward = env.get_reward()\n","            total_reward += reward\n","            agent.update_q_value(current_position, action, reward, env.agent_position)\n","\n","        if episode % 100 == 0:\n","            print(f\"Episode: {episode}, Total Reward: {total_reward}\")"],"metadata":{"id":"xcFCviZvjCsc","executionInfo":{"status":"ok","timestamp":1701286673423,"user_tz":300,"elapsed":311,"user":{"displayName":"Eugene Tye","userId":"03176183298028220664"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Test the Q-Learning Agent"],"metadata":{"id":"zMFwSBh2lJgv"}},{"cell_type":"markdown","source":["* Here, a function to test the trained Q-learning agent is defined\n","* The agent interacts with the environment without exploration (epsilon=0), and the current position and chosen action are printed\n","\n"],"metadata":{"id":"bVec7gtTlMpR"}},{"cell_type":"code","source":["def test_q_learning_agent(agent, env):\n","    env.reset()\n","    print(\"\\nTesting the trained agent:\")\n","    while not env.is_terminal() and not env.is_pitfall():\n","        action = agent.choose_action(env.agent_position, epsilon=0)  # No exploration during testing\n","        env.take_action(action)\n","        print(f\"Current Position: {env.agent_position}, Chosen Action: {action}\")"],"metadata":{"id":"mZJT_PhQjGW0","executionInfo":{"status":"ok","timestamp":1701286673423,"user_tz":300,"elapsed":9,"user":{"displayName":"Eugene Tye","userId":"03176183298028220664"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Execution"],"metadata":{"id":"xAtsGHZulXDZ"}},{"cell_type":"markdown","source":["* Here, the script is executed as the main program\n","* It creates an instance of the grid world and the Q-learning agent, trains the agent, and then tests its performance in the environment\n","* The epsilon parameter controls exploration during training"],"metadata":{"id":"R8W0MNQElZRl"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    world = GridWorld()\n","    agent_actions = ['up', 'down', 'left', 'right']\n","    q_agent = QLearningAgent(actions=agent_actions)\n","\n","    # Training the agent\n","    train_q_learning_agent(q_agent, world, epsilon=0.2, num_episodes=1000)\n","\n","    # Testing the agent\n","    test_q_learning_agent(q_agent, world)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0nfSjwqjIYO","executionInfo":{"status":"ok","timestamp":1701286673423,"user_tz":300,"elapsed":8,"user":{"displayName":"Eugene Tye","userId":"03176183298028220664"}},"outputId":"813c63f2-5a32-4814-9f80-83dc452b8b15"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 0, Total Reward: -1\n","Episode: 100, Total Reward: 1\n","Episode: 200, Total Reward: 1\n","Episode: 300, Total Reward: 1\n","Episode: 400, Total Reward: -1\n","Episode: 500, Total Reward: 1\n","Episode: 600, Total Reward: 1\n","Episode: 700, Total Reward: 1\n","Episode: 800, Total Reward: 1\n","Episode: 900, Total Reward: 1\n","\n","Testing the trained agent:\n","Current Position: (0, 1), Chosen Action: right\n","Current Position: (0, 2), Chosen Action: right\n","Current Position: (0, 3), Chosen Action: right\n","Current Position: (1, 3), Chosen Action: down\n","Current Position: (2, 3), Chosen Action: down\n","Current Position: (3, 3), Chosen Action: down\n"]}]}]}